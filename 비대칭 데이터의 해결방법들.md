# 비대칭 데이터의 해결방법들



#### 은행이나 카드회사에서 머신러닝을 통해 사기거래를 감지해야할때, 

#### 하루에 누적되는 정상거래데이터는 셀수없이 많지만, 사기거래는 가뭄에 콩나듯 적다.

#### 이러한 불균형 데이터를  통해 학습을 진행할 경우, 제대로 학습할까?

#### 사기거래데이터는 아마 노이즈로 취급되어 무시당하고 머신은 정상거래만 리턴할것이다.

#### 그렇다면 비대칭 데이터의 문제는 어떻게 해결 할 수 있을까?



## 1.UNDERSAMPLING

소수클래스(사기거래)데이터와 다수클래스(정상거래)데이터 중 다수클래스의 데이터를 줄여 비율을 맞추는방식



#### 	1)Tomek's link method(토멕링크)

​		토멕링크는 서로 다른 클래스의 데이터중 서로에게 가장 가까운 한쌍을 말한다.

​		토멕링크방법은 토멕링크를 찾은 뒤, 다수클래스에 해당하는 데이터를 제거 하는 방식으로 비율을 맞춘다

​		제거 과정은 모든 최소거리 연결쌍이 같은 클래스에 있을때 까지 계속 된다

​		직관적으로 생각했을 때, 클래스를  분류하기 애매한 데이터를 지우며 언더샘플링을 하기 때문에 노이즈를 

​		제거하고 경계선을 뚜렷하게 한다는 장점이 있지만, 소수 클래스의 경계선 범위가 넓어지기 때문에 

​		반드시 정확도가 높아진다고 장담할 순 없다.



![ig](https://user-images.githubusercontent.com/44566113/57902588-fdd3da00-78a4-11e9-8aff-f319b17dc960.JPG)



#### 	2)Condensed Nearest Neighbour

​		첫번째로 가장 가까운 데이터(1-Nearest Neighbour)를 참고하여 남길지 여부를 결정한다.

​			i) 우선 undersampling 이기 때문에 소수클래스 데이터는 모두 유지한다.

​			ii) 그 후 다수클래스데이터 중 하나를 선택 한 뒤 그 데이터와 가장 가까운 거리의 데이터를 찾는다

​				(유클리디안 거리를 이용한다!)

​			iii) 가장 가까운데이터의 클래스가 동일할 경우, 즉 다수 클래스일 경우, ii)에서 선택한 데이터를 제거한다

​			iiii) 가장 가까운데이터의 클래스가 다를 경우, 즉 소수클래스일 경우, ii)에서 선택한 데이터를 유지한다

​		ii) ~iiii) 과정을 다수클래스의 모든데이터에 적용시키며 다수클래스 데이터를 줄여 나간다.

​		직관적으로 생각했을 때, 소수클래스와 가까이 있는 다수클래스데이터만 남기 때문에 

​		사라지는 다수클래스 데이터가 많아 질것이다. 또한, 소수클래스와 다수클래스가 모두 가까이 있어서

​		경계선을 찾는데 어려움이 생길것 같다. 통알못의 얕은 이해로는 장점을 찾지 못한 방법이다.

![condensedNN](https://user-images.githubusercontent.com/44566113/57902589-fdd3da00-78a4-11e9-8caa-497d72f22411.JPG)

#### 3)One Sided Selection

​	토멕링크방법과 Condensed Nearest Neighbour 를 합친 방식이다.

​	우선 토멕링크를 통해 근접해 있는 다수클래스데이터를 지우고,

​	Condensed NN을 통해 소수클래스데이터와 멀리 위치한 다수클래스 데이터를 제거한다.

​	균형 잡힌 방식이라 생각한다.



![onesided](https://user-images.githubusercontent.com/44566113/57902590-fe6c7080-78a4-11e9-8067-cec2d895dfc7.JPG)



#### 4)Edited Nearest Neighbours

​	다수클래스의 데이터 중 가장 가까운 k 개의 데이터가 모두() or 다수(최빈값)이 아니면 데이터를 삭제하는 방식

​	여기서`모두(kind_sell='all') or 다수(kind_sel='mode')`는 사용자가 정하는 파라미터 값이다. 

​	토멕링크와 마찬가지로 소수클래스 데이터주변에 위치한 다수클래스데이터가 제거될 확률이 높아지지만,

​	소수클래스와 가까이 있는 다수클래스데이터가 무조건 제거됬던것과 달리, 떼거지로 뭉쳐있으면

​	제거되지 않는 특징이 있다.

​	직관적으로 판단했을때 토멕링크는 가까이 붙어있으면 제거하기 때문에 노이즈 데이터가 제거될 수도있지만,

​	노이즈가 아닌 단순히 가까이 위치한 데이터가 제거될 가능성도 있다.

​	일반적으로 생각했을때, 노이즈가 아닌 데이터라면 주변에 위치한 다수의 사례가 함께 있을 것으로 생각한다.

​	때문에 Edited Nearest Neighbour 방법은 다수클래스 데이터가 소수클래스와 가까이 있어도 주변에 다수가 

​	분포되어 있으면 제거 되지 않기 때문에 중요한 정보를 살리는 방법이라 생각한다. 

​	

​	사진은 `kind_sell='all'` 파라미터로 제거한 자료:)

![enn](https://user-images.githubusercontent.com/44566113/57902591-fe6c7080-78a4-11e9-8b04-8b5fefa96b0a.JPG)

​		

## OVERSAMPLING



#### 	1) Ramdom Oversampling

​		소수클래스 데이터를 복사하여 추가하는 방법이다. 단순하게 동일한 데이터 양을 늘리기 때문에 가중치를

​		증가시키는 작업이라 할 수 있다. 때문에 시각화에선 아무리 복사해도 티가 안난다.

![randomOversampling](https://user-images.githubusercontent.com/44566113/57902592-fe6c7080-78a4-11e9-856c-f2d5a11aae66.JPG)

#### 	2) SMOTE(Synthetic Minority Over-sampling Technique)

​		소수클래스 데이터 사이에 임의의 데이터를 추가하는 방식이다.

​		i ) 소수클래스의 데이터 중 하나를 선택 한 뒤 그 데이터와 가장 가까운(Nearest Neighbor) k 개의

​			소수클래스 데이터를 찾습니다.

​		ii)  찾은 k 개의 소수클래스 데이터 중 임의로 하나를 선택 한다.

​		iii) 처음 i)에서 선택 한 데이터와 ii)에서 선택한 데이터 사이에 임의의 데이터를 생성한다.



​	 ![adasynì](https://cdn-images-1.medium.com/max/1200/1*6UFpLFl59O9e3e38ffTXJQ.png)

#### 	3) Borderline-SMOTE

​		SMOTE는 효과가 좋지만 일반화(over generalization)의 문제를 갖고있다. 쉽게 얘기하면 Nearest Neighbor 

​		를 찾는 과정에서 주변에 있는 `소수클래스 데이터만 고려`하기 때문에 데이터를 생성하는 과정에서

​		중첩(overlapping occurrence)이 발생한다, 즉 다수클래스 데이터와 소수클래스 데이터가 겹치게 되어 

​		학습성능을 저하시킨다. 이러한 문제를 해결하기 위해 고안된 기법이다.

​	

중첩된 사례(그림 가운데에 있는 노이즈 하나를 통해 만든  generated sample들이 모두 중첩되었다 )

​		![SMOTE_OVERLAPPING](https://user-images.githubusercontent.com/44566113/57902587-fdd3da00-78a4-11e9-8927-545a1124f82e.JPG)

​		기본적인 방식은 동일하지만 k 개의 Nearest Neighbor 를 찾을 때 

​		`소수,다수클래스 구분하지 않고` 모두 포함시킨다.

​		그 뒤, 포함된 다수클래스 데이터의 수가 k/2  보다 크고 k 보다 작을 경우  'DANGER ' 로 판단하고 

​		데이터를 생성한다

​		포함된 다수클래스 데이터의 수가 k 와 동일할 경우 'NOISE' 로 판단하고 데이터를 생성하지 않는다.

​		이 외에 경우는 'SAFE' 로 판단하고 데이터를 생성한다



![IMG](http://2.bp.blogspot.com/-aACF0FTDOQ8/UtjG6WYYpKI/AAAAAAAAAL4/vyn2c1qA5CE/s1600/a2.jpg)



​	



#### 	3) ADASYN(Adaptive Synthetic Sampling)

​		SMOTE 의 중첩문제를 해결하기 위한 또 다른 기법이다.

​		위 방법과 큰 차이는 주위 데이터 분포에 따라 발생시킬 합성데이터의 수를 체계적으로 조절한다는점이다

​		

​		i) 첫번째로, 생성할 데이터의 수를 계산한다.

###### 			`G = (다수클래스데이터의 전체 수 - 소수클래스의 전체 수) * beta`

​			여기서 beta는 0~1 사이의 값으로 두 그룹간 균형을 조절하기 위해 정하는 숫자다.

​		ii) 소수클래스 데이터 x(i) 에 대해 Borderline SMOTE 와 동일하게 K-NN을 찾고,  Gamma를 계산한다

###### 			`Gamma(i) = (delta(i)/K)/Z,  	i = 1,2..|Smin|(소수클래스 전체 수)`

​			`delta(i)`는 x(i)의 K-NN 중에서 다수클래스에 속하는 데이터의 수다. 

​			`Z`는 Gamma(i) 가 p.d.f(probability density function)가 되도록하는 정규화상수다 (sigma(Gamma(i)) = 1)

​		iii) 소수클래스 데이터 x(i) 에 대해 생성할 데이터 수를 결정한다

###### 			`g(i) = Gamma(i) * G `

​		iiii) 마지막으로 소수클래스 데이터 x(i)에 대해 SMOTE 방식으로 g(i)개의 데이터를 생성한다.

​		

​		위 식에서 delta(i)가 크면 많은 데이터를 발생 시킨다. 즉 S**maj**에 속하는 샘플의 수가 많은 x**i**가 많은 

​		데이터를 발생시킨다. 

​		ADYSYN의 주요 아이디어는 합성 데이터의 수를 자동적으로 결정하기 위한 표준으로 p.d.f인 Gama를 

​		사용하는 것이다.  





<https://datascienceschool.net/view-notebook/c1a8dad913f74811ae8eef5d3bedc0c3/>

<https://medium.com/coinmonks/smote-and-adasyn-handling-imbalanced-data-set-34f5223e167>

###### Felix Last, Georgios Douzas and Fernando Bacao 'Oversampling for Imbalanced Learning Based on K-Means and SMOTE'

###### H. Han, et.al., Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning, Proc. ICIC, 2005.